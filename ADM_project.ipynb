{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "#import urllib.request\n",
    "#import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-4f4b56d23edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/jponder/Documents/School Notes/Advanced Data Mining/rows.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#d = pd.read_json() # the consumer dataset is now a Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jponder/anaconda/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jponder/anaconda/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jponder/anaconda/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Read the input\n",
    "file = \"/Users/jponder/Documents/School Notes/Advanced Data Mining/rows.json\"\n",
    "d = d = json.loads(file)\n",
    "\n",
    "#d = pd.read_json() # the consumer dataset is now a Pandas DataFrame\n",
    "# Only interested in data with consumer complaints\n",
    "#d = d[d['Consumer complaint narrative'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(d['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    #return re.findall('[\\w\\-]+', my_string.lower())\n",
    "    #\\W -> Matches any non-alphanumeric character; \n",
    "    #this is equivalent to the class [^a-zA-Z0-9_]. \n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "    no_numbers = my_string.lower().translate({ord(ch): None for ch in '0123456789'})\n",
    "    word_list = re.sub('\\W+', ' ', no_numbers).split()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    output = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word != \"xxxx\" and word not in stop:\n",
    "            word = stemmer.stem(word)\n",
    "            if len(word) > 2:\n",
    "                output.append(word)\n",
    "            \n",
    "    word_list = output\n",
    "    \n",
    "    # preprocessing ->\n",
    "   \n",
    "    # remove too low and too high frequency words -> we can't calculate here\n",
    "\n",
    "    return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    # step 1 -> do tokenize_string for each row in movies['genres']\n",
    "\n",
    "    all_words = []\n",
    "    for row in data['Consumer complaint narrative']:\n",
    "        #genre_list = re.sub(r'[||)|(]', r' ',row.lower()).split()\n",
    "        word_list = tokenize_string(row)\n",
    "        #print(word_list)\n",
    "        #print(len(genre_list))\n",
    "        all_words.append(word_list)\n",
    "    \n",
    "    # step 2 -> add column tokens in movies\n",
    "    array = np.array(all_words)\n",
    "    \n",
    "    #print(array[:5])\n",
    "    #print('#list = ',len(array))\n",
    "    \n",
    "    new_data = d.assign(tokens = array)\n",
    "    \n",
    "    #print(new_movies.head(5))\n",
    "    return(new_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(data):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "   \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print(movies[:5]) \n",
    "    \n",
    "    #step 1 -> build a vocab and df(term)\n",
    "    vocab = {}\n",
    "    vocab_list = []\n",
    "    df = {}\n",
    "    \n",
    "    for tokenization in data['tokens']:\n",
    "        tokens = list(set(tokenization))\n",
    "        for term in tokens:\n",
    "            if term not in vocab.keys():\n",
    "                vocab.setdefault(term,-1)\n",
    "             \n",
    "            if term not in df.keys(): \n",
    "                df.setdefault(term,1)\n",
    "            else :\n",
    "                df[term] += 1\n",
    "             \n",
    "             \n",
    "    #print('vocab = ', vocab)\n",
    "    \n",
    "    vocab_list = sorted(vocab.keys(), key = lambda x:x)\n",
    "    #print('vocab_list = ', vocab_list)\n",
    "    \n",
    "    for i,term in enumerate(vocab_list):\n",
    "         vocab[term] = i\n",
    "            \n",
    "    #        \n",
    "         \n",
    "    #print('Sorted vocab = ', sorted(vocab.items()))\n",
    "    #print('df = ',sorted(df.items(), key=lambda x:x[0]))\n",
    "    \n",
    "    # step 2 -> Build a csr_matrix for each row of movies['tokens']\n",
    "   \n",
    "    #print('N = ',N)\n",
    "    \n",
    "    #[comedy, comedy, comedy, horror]  -> max_k tf(k, d) = 3 \n",
    "    #[action, comedy,thriller] -> tf(action, d) =1\n",
    "    # df(i) ->\n",
    "    #num_features is the total number of unique features across all documents.\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    csr_array =[]\n",
    "    \n",
    "    for row1 in data['tokens']:\n",
    "        csr_row = []\n",
    "        csr_col = []\n",
    "        csr_data = []\n",
    "        max_k = 0\n",
    "       \n",
    "        max_k = Counter(row1).most_common()[:1][0][1]\n",
    "        row = list(set(row1))\n",
    "\n",
    "        #print('removed duplicates =',row)\n",
    "        for term in row:       \n",
    "            csr_row.append(0)\n",
    "            csr_col.append(vocab[term])\n",
    "            #tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "            tf = Counter(row1)[term]\n",
    "            #max_k = max_k.most_common()[:1][0][1]\n",
    "         \n",
    "            #print('term = %s ---> tf = %d ---> max_k = %d'%(term,tf,max_k))\n",
    "            tfidf = (tf / max_k) * math.log10(N/df[term])\n",
    "            csr_data.append(tfidf)\n",
    "           \n",
    "         \n",
    "        #print('csr_row = ',csr_row) \n",
    "        #print('csr_col = ',csr_col)\n",
    "        #print('csr_data=',csr_data)\n",
    "        X = csr_matrix((csr_data, (csr_row, csr_col)), shape=(1, len(vocab)), dtype=np.float64)\n",
    "       \n",
    "        #print('X ->\\n',X.toarray())\n",
    "        csr_array.append(X)\n",
    "    \n",
    "\n",
    "    # step 3 -> add column features to movies \n",
    "    #print('size of csr_array = ',len(csr_array)) \n",
    "    #print('CSR = ',csr_array[:2])  \n",
    "    new_data = data.assign(features = csr_array)\n",
    "    #print(new_movies.head(2))\n",
    "     \n",
    "    return(new_data,vocab)   \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.head(20)['tokens'])\n",
    "print(data.tail(20)['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, vocab = featurize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('vocab:')\n",
    "print(sorted(vocab.items())[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
