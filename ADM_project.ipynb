{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "#import urllib.request\n",
    "#import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swapnil\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read the input\n",
    "d = pd.read_csv(\"F:\\IITC\\ADM\\Extra\\consumer_complaints.csv\") # the consumer dataset is now a Pandas DataFrame\n",
    "# Only interested in data with consumer complaints\n",
    "d = d[d['Consumer complaint narrative'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56777     Received Capital One charge card offer XXXX. A...\n",
      "56834     I do n't know how they got my cell number. I t...\n",
      "56884     I 'm a longtime member of Charter One Bank/RBS...\n",
      "56894     After looking at my credit report, I saw a col...\n",
      "56898     I received a call from a XXXX XXXX from XXXX @...\n",
      "56910     Was not contacted 4 years later about some pri...\n",
      "56911     Collection Consultants is reporting a collecti...\n",
      "56914     I had my purse stolen in 2007. They never foun...\n",
      "56917     I attempted to apply for a Discover Card Onlin...\n",
      "56918     Continued attempts by XXXX XXXX XXXX to collec...\n",
      "56920     This is a continuation of a previous issue wit...\n",
      "56928     Going through a divorce, my ex and I were unab...\n",
      "56930     I am assisted with my mortgage through an agen...\n",
      "56939     i submitted a payment on XXXX XXXX 2015 for my...\n",
      "56940     I recieved a notice from Midland Credit Manage...\n",
      "56956     XXXX Card services was bought out by Capital O...\n",
      "56965     I was reported late by Discover Card to the re...\n",
      "56982     XXXX i receive an email from citibank regardin...\n",
      "56983     I agreed to a settlement arrangement on a acco...\n",
      "56985     Green Tree Financial somehow got XXXX Loan AFT...\n",
      "56989     I took out a Loan from Cash Central XXXX, Al f...\n",
      "56990     I filed ChapterXXXX bankruptcy on my accounts ...\n",
      "56995     To whom it may concern, To those that follow t...\n",
      "56996     I 'm no longer an account holder of SunTrust. ...\n",
      "57004     I 've been receiving numerous robocalls, hang-...\n",
      "57007     My mortgage servicer, Wells Fargo, did not giv...\n",
      "57012     Please investigate the practices of Experian. ...\n",
      "57014     Portfolio Recovery does not disclose who is ca...\n",
      "57022     After banking with Citibank for over 3 years, ...\n",
      "57023     I am a victim of a parked debt bought by a deb...\n",
      "                                ...                        \n",
      "669665    There was an account opened with XXXX I have n...\n",
      "669670    Fastunsecured.com submitted my information to ...\n",
      "669675    Green Tree Servicing failed to honor my perman...\n",
      "669685    Opened checking account with valid funds. Remo...\n",
      "669693    Disputed the XXXX bankruptcies showing on my r...\n",
      "669694    I was evaluating Wells Fargo Home Mortgage to ...\n",
      "669702    I have sent Equifax several letters over 3 mon...\n",
      "669715    We have a mortgage with Loan Care, a XXXX Comp...\n",
      "669716    Ditech Financial has misapplied payments we ha...\n",
      "669721    I 'm XXXX, disalbled, after an accident at wor...\n",
      "669723    Fedloan Servicing, a Department of Education S...\n",
      "669727    We went to refinance with another company due ...\n",
      "669730    I have asked 21st Century Mortgage twice to st...\n",
      "669731    I purchased a timeshare and in 2012 defaulted ...\n",
      "669732    Edward Jones XXXX billed my account for {$440....\n",
      "669739    On XX/XX/2016 I made a deposit of {$900.00} vi...\n",
      "669742    I closed my savings and checking account at Ke...\n",
      "669743    I have sent letters to the credit bureaus as w...\n",
      "669746    On XXXX XXXX, 2016, I sent a letter to the cre...\n",
      "669748    I was a victim of identity theft and noticed a...\n",
      "669749    American Express has charged me an annual fee ...\n",
      "669754    PNC Mortgage charged me {$650.00} for an appra...\n",
      "669758    I have been working with clients for 8 months,...\n",
      "669759    I have a loan through first national bank of o...\n",
      "669761    Chase put a negative account on my credit file...\n",
      "669763    My mortgage company has tried twice now to add...\n",
      "669769    This is an additional complaint to the previou...\n",
      "669771    I have been a customer with Capitalone for ove...\n",
      "669772    I am paying off my loan by making largish paym...\n",
      "669776    I have been in a battle to make mortgage payme...\n",
      "Name: Consumer complaint narrative, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(d['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Date received           Product                            Sub-product  \\\n",
      "56834     3/23/2015   Debt collection  Other (i.e. phone, health club, etc.)   \n",
      "56884     3/23/2015       Credit card                                    NaN   \n",
      "56894     3/23/2015  Credit reporting                                    NaN   \n",
      "56898     3/23/2015   Debt collection                            Payday loan   \n",
      "\n",
      "                                        Issue  \\\n",
      "56834     Improper contact or sharing of info   \n",
      "56884                                 Rewards   \n",
      "56894  Incorrect information on credit report   \n",
      "56898     Improper contact or sharing of info   \n",
      "\n",
      "                                   Sub-issue  \\\n",
      "56834      Contacted me after I asked not to   \n",
      "56884                                    NaN   \n",
      "56894                Information is not mine   \n",
      "56898  Talked to a third party about my debt   \n",
      "\n",
      "                            Consumer complaint narrative  \\\n",
      "56834  I do n't know how they got my cell number. I t...   \n",
      "56884  I 'm a longtime member of Charter One Bank/RBS...   \n",
      "56894  After looking at my credit report, I saw a col...   \n",
      "56898  I received a call from a XXXX XXXX from XXXX @...   \n",
      "\n",
      "                                Company public response  \\\n",
      "56834                                               NaN   \n",
      "56884                                               NaN   \n",
      "56894  Company chooses not to provide a public response   \n",
      "56898                                               NaN   \n",
      "\n",
      "                              Company State ZIP code Tags  \\\n",
      "56834    CCS Financial Services, Inc.    AR    727XX  NaN   \n",
      "56884  Citizens Financial Group, Inc.    MI    482XX  NaN   \n",
      "56894                        Experian    FL    331XX  NaN   \n",
      "56898          Big Picture Loans, LLC    SC    291XX  NaN   \n",
      "\n",
      "      Consumer consent provided? Submitted via Date sent to company  \\\n",
      "56834           Consent provided           Web            3/23/2015   \n",
      "56884           Consent provided           Web            3/23/2015   \n",
      "56894           Consent provided           Web            3/27/2015   \n",
      "56898           Consent provided           Web            3/23/2015   \n",
      "\n",
      "          Company response to consumer Timely response? Consumer disputed?  \\\n",
      "56834          Closed with explanation              Yes                 No   \n",
      "56884          Closed with explanation              Yes                Yes   \n",
      "56894  Closed with non-monetary relief              Yes                 No   \n",
      "56898          Closed with explanation              Yes                 No   \n",
      "\n",
      "       Complaint ID  \n",
      "56834     1296593.0  \n",
      "56884     1296693.0  \n",
      "56894     1296955.0  \n",
      "56898     1296727.0  \n"
     ]
    }
   ],
   "source": [
    "print(d[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    #return re.findall('[\\w\\-]+', my_string.lower())\n",
    "    #\\W -> Matches any non-alphanumeric character; \n",
    "    #this is equivalent to the class [^a-zA-Z0-9_]. \n",
    "       \n",
    "    word_list = re.sub('\\W+', ' ', my_string.lower()).split()\n",
    "    \n",
    "    # preprocessing ->\n",
    "    # remove stop words\n",
    "    # remove numbers\n",
    "   \n",
    "    # remove too low and too high frequency words -> we can't calculate here\n",
    "\n",
    "    return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    # step 1 -> do tokenize_string for each row in movies['genres']\n",
    "\n",
    "    all_words = []\n",
    "    for row in data['Consumer complaint narrative']:\n",
    "        #genre_list = re.sub(r'[||)|(]', r' ',row.lower()).split()\n",
    "        word_list = tokenize_string(row)\n",
    "        #print(word_list)\n",
    "        #print(len(genre_list))\n",
    "        all_words.append(word_list)\n",
    "    \n",
    "    # step 2 -> add column tokens in movies\n",
    "    array = np.array(all_words)\n",
    "    \n",
    "    #print(array[:5])\n",
    "    #print('#list = ',len(array))\n",
    "    \n",
    "    new_data = d.assign(tokens = array)\n",
    "    \n",
    "    #print(new_movies.head(5))\n",
    "    return(new_data)  \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(data):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "   \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print(movies[:5]) \n",
    "    \n",
    "    #step 1 -> build a vocab and df(term)\n",
    "    vocab = {}\n",
    "    vocab_list = []\n",
    "    df = {}\n",
    "    \n",
    "    for row1 in data['tokens']:\n",
    "        row = list(set(row1))\n",
    "        for term in row:\n",
    "            if term not in vocab.keys():\n",
    "                vocab.setdefault(term,-1)\n",
    "             \n",
    "            if term not in df.keys(): \n",
    "                df.setdefault(term,1)\n",
    "            else :\n",
    "                df[term] += 1\n",
    "             \n",
    "             \n",
    "    #print('vocab = ', vocab)\n",
    "    \n",
    "    vocab_list = sorted(vocab.keys(), key = lambda x:x)\n",
    "    #print('vocab_list = ', vocab_list)\n",
    "    \n",
    "    for i,term in enumerate(vocab_list):\n",
    "         vocab[term] = i\n",
    "            \n",
    "    #        \n",
    "         \n",
    "    #print('Sorted vocab = ', sorted(vocab.items()))\n",
    "    #print('df = ',sorted(df.items(), key=lambda x:x[0]))\n",
    "    \n",
    "    # step 2 -> Build a csr_matrix for each row of movies['tokens']\n",
    "    N = len(data)\n",
    "    #print('N = ',N)\n",
    "    \n",
    "    #[comedy, comedy, comedy, horror]  -> max_k tf(k, d) = 3 \n",
    "    #[action, comedy,thriller] -> tf(action, d) =1\n",
    "    # df(i) ->\n",
    "    #num_features is the total number of unique features across all documents.\n",
    "    \n",
    "    csr_array =[]\n",
    "    \n",
    "    for row1 in data['tokens']:\n",
    "        csr_row = []\n",
    "        csr_col = []\n",
    "        csr_data = []\n",
    "        max_k = 0\n",
    "       \n",
    "        max_k = Counter(row1).most_common()[:1][0][1]\n",
    "        row = list(set(row1))\n",
    "\n",
    "        #print('removed duplicates =',row)\n",
    "        for term in row:       \n",
    "            csr_row.append(0)\n",
    "            csr_col.append(vocab[term])\n",
    "            #tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "            tf = Counter(row1)[term]\n",
    "            #max_k = max_k.most_common()[:1][0][1]\n",
    "         \n",
    "            #print('term = %s ---> tf = %d ---> max_k = %d'%(term,tf,max_k))\n",
    "            tfidf = (tf / max_k) * math.log10(N/df[term])\n",
    "            csr_data.append(tfidf)\n",
    "           \n",
    "         \n",
    "        #print('csr_row = ',csr_row) \n",
    "        #print('csr_col = ',csr_col)\n",
    "        #print('csr_data=',csr_data)\n",
    "        X = csr_matrix((csr_data, (csr_row, csr_col)), shape=(1, len(vocab)), dtype=np.float64)\n",
    "       \n",
    "        #print('X ->\\n',X.toarray())\n",
    "        csr_array.append(X)\n",
    "    \n",
    "\n",
    "    # step 3 -> add column features to movies \n",
    "    #print('size of csr_array = ',len(csr_array)) \n",
    "    #print('CSR = ',csr_array[:2])  \n",
    "    new_data = data.assign(features = csr_array)\n",
    "    #print(new_movies.head(2))\n",
    "     \n",
    "    return(new_data,vocab)   \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, vocab = featurize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:\n",
      "[('0', 0), ('00', 1), ('000', 2), ('0000', 3), ('00000', 4), ('0001', 5), ('000a', 6), ('000dollars', 7), ('000if', 8), ('000ii', 9), ('000ins', 10), ('000s', 11), ('000xx', 12), ('001', 13), ('0018', 14), ('001a', 15), ('001b', 16), ('001c', 17), ('002', 18), ('003', 19), ('004', 20), ('0041', 21), ('0065', 22), ('0077', 23), ('0080', 24), ('0085', 25), ('009', 26), ('00additional', 27), ('00amount', 28), ('00balance', 29), ('00but', 30), ('00eventually', 31), ('00for', 32), ('00i', 33), ('00interest', 34), ('00lender', 35), ('00purchase', 36), ('00time', 37), ('00total', 38), ('00usd', 39), ('00xx', 40), ('00xxxx', 41), ('01', 42), ('0107', 43), ('0116', 44), ('012', 45), ('01231', 46), ('015', 47), ('016', 48), ('017', 49), ('02', 50), ('020', 51), ('021', 52), ('02240', 53), ('025', 54), ('027', 55), ('03', 56), ('030', 57), ('031', 58), ('034', 59), ('036', 60), ('038', 61), ('04', 62), ('040', 63), ('043', 64), ('0432', 65), ('05', 66), ('056', 67), ('06', 68), ('0608', 69), ('069', 70), ('07', 71), ('070', 72), ('07732', 73), ('078', 74), ('07800', 75), ('08', 76), ('080', 77), ('0804', 78), ('0805', 79), ('081', 80), ('0813', 81), ('09', 82), ('0945', 83), ('09x', 84), ('0cwen', 85), ('0date', 86), ('0f', 87), ('0high', 88), ('0highest', 89), ('0k', 90), ('0monthly', 91), ('0n', 92), ('0ne', 93), ('0nxxxx', 94), ('0payment', 95), ('0r', 96), ('0recent', 97), ('0remarks', 98), ('0responsibility', 99), ('0ther', 100), ('0times', 101), ('0ver', 102), ('0wed', 103), ('0xxxx', 104), ('1', 105), ('10', 106), ('100', 107), ('1000', 108), ('10000', 109), ('100000', 110), ('1000000', 111), ('10000000', 112), ('1000s', 113), ('1001', 114), ('1002', 115), ('1003', 116), ('1005', 117), ('100k', 118), ('100percent', 119), ('100s', 120), ('100th', 121), ('100x', 122), ('100xxxx', 123), ('100years', 124), ('101', 125), ('1013', 126), ('1016', 127), ('102', 128), ('1020', 129), ('1022', 130), ('1024', 131), ('1025', 132), ('1026', 133), ('1028', 134), ('1029', 135), ('103', 136), ('1031', 137), ('1036', 138), ('104', 139), ('105', 140), ('1053', 141), ('1055', 142), ('106', 143), ('107', 144), ('108', 145), ('1080a', 146), ('1087cc', 147), ('109', 148), ('10916', 149), ('1095a', 150), ('1098', 151), ('1099', 152), ('1099c', 153), ('10_statement', 154), ('10compulsory', 155), ('10day', 156), ('10days', 157), ('10k', 158), ('10min', 159), ('10mo', 160), ('10mos', 161), ('10s', 162), ('10th', 163), ('10ths', 164), ('10x', 165), ('10xs', 166), ('10xxxx', 167), ('10y', 168), ('10year', 169), ('10years', 170), ('10yr', 171), ('10yrs', 172), ('11', 173), ('110', 174), ('1100', 175), ('11000', 176), ('110000', 177), ('1103', 178), ('111', 179), ('111th', 180), ('112', 181), ('1120', 182), ('113', 183), ('1137', 184), ('113th', 185), ('114', 186), ('1140', 187), ('1141', 188), ('115', 189), ('116', 190), ('117', 191), ('1178', 192), ('118', 193), ('1189', 194), ('119', 195), ('11_statement', 196), ('11a', 197), ('11days', 198), ('11k', 199), ('11months', 200), ('11mos', 201), ('11straight', 202), ('11th', 203), ('11years', 204), ('11yrs', 205), ('12', 206), ('120', 207), ('1200', 208), ('12000', 209), ('120000', 210), ('1204', 211), ('120days', 212), ('120th', 213), ('120xxxx', 214), ('121', 215), ('1216', 216), ('122', 217), ('1220', 218), ('123', 219), ('123110', 220), ('124', 221), ('1241', 222), ('124k', 223), ('125', 224), ('1250', 225), ('1261', 226), ('127', 227), ('128', 228), ('129', 229), ('129b', 230), ('129c', 231), ('12_statement', 232), ('12a', 233), ('12b', 234), ('12business', 235), ('12cfr1024', 236), ('12k', 237), ('12mo', 238), ('12month', 239), ('12months', 240), ('12mos', 241), ('12mths', 242), ('12q', 243), ('12th', 244), ('12x', 245), ('12years', 246), ('12yrs', 247), ('13', 248), ('130', 249), ('1300', 250), ('13000', 251), ('130000', 252), ('1308', 253), ('131', 254), ('1317', 255), ('1319', 256), ('132', 257), ('1321', 258), ('1322', 259), ('133', 260), ('134', 261), ('13401', 262), ('13407', 263), ('1341', 264), ('1342', 265), ('1345', 266), ('135', 267), ('1350', 268), ('135k', 269), ('1361', 270), ('137', 271), ('1370', 272), ('1376', 273), ('138', 274), ('1382a', 275), ('1383', 276), ('1385', 277), ('139', 278), ('1391', 279), ('13a', 280), ('13th', 281), ('13xxxx', 282), ('13years', 283), ('14', 284), ('140', 285), ('1400', 286), ('14000', 287), ('140000', 288), ('140b', 289), ('141', 290), ('1411', 291), ('1412', 292), ('143', 293), ('1431', 294), ('144', 295), ('1442', 296), ('1446', 297), ('144a', 298), ('145', 299), ('146', 300), ('1463', 301), ('1464', 302), ('147', 303), ('1470', 304), ('1474', 305), ('148', 306), ('149', 307), ('14days', 308), ('14mins', 309), ('14months', 310), ('14more', 311), ('14th', 312), ('14x', 313), ('14xxxx', 314), ('14years', 315), ('14yrs', 316), ('15', 317), ('150', 318), ('1500', 319), ('15000', 320), ('150000', 321), ('15005', 322), ('1501', 323), ('150days', 324), ('150k', 325), ('151', 326), ('1512', 327), ('1513', 328), ('1522', 329), ('153', 330), ('1530', 331), ('1541', 332), ('155', 333), ('1550', 334), ('156', 335), ('1563', 336), ('1567', 337), ('1571', 338), ('1572', 339), ('1573', 340), ('1574', 341), ('1575', 342), ('1576', 343), ('1577', 344), ('1578', 345), ('157812', 346), ('1578penal', 347), ('158', 348), ('159', 349), ('1594', 350), ('15946', 351), ('15all', 352), ('15am', 353), ('15as', 354), ('15days', 355), ('15ish', 356), ('15k', 357), ('15loan', 358), ('15min', 359), ('15mins', 360), ('15minutes', 361), ('15nationstar', 362), ('15representative', 363), ('15rushmore', 364), ('15th', 365), ('15the', 366), ('15u', 367), ('15usc', 368), ('15usc1681', 369), ('15usc1692', 370), ('15usc1692a', 371), ('15usc1692c', 372), ('15usc1692d', 373), ('15usc1692e', 374), ('15usc1692f', 375), ('15usc1692g', 376), ('15usc1692j', 377), ('15usc1692k', 378), ('15usc1692l', 379), ('15usca', 380), ('15we', 381), ('15x', 382), ('15xxxx', 383), ('15y', 384), ('15year', 385), ('15years', 386), ('15yr', 387), ('15yrs', 388), ('16', 389), ('160', 390), ('1600', 391), ('16000', 392), ('160000', 393), ('1601', 394), ('1602', 395), ('1605', 396), ('161', 397), ('1611', 398), ('162', 399), ('1621', 400), ('1625', 401), ('1628', 402), ('1629e', 403), ('163', 404), ('1631', 405), ('1635', 406), ('1637', 407), ('1637a', 408), ('1639', 409), ('1639g', 410), ('164', 411), ('1640', 412), ('1641', 413), ('1641f1', 414), ('1642', 415), ('1643', 416), ('165', 417), ('16580', 418), ('166', 419), ('1666', 420), ('1666c', 421), ('1666i', 422), ('1667', 423), ('1667f', 424), ('1669', 425), ('167', 426), ('1679', 427), ('168', 428), ('1681', 429), ('16816', 430), ('1681a', 431), ('1681and', 432), ('1681b', 433), ('1681c', 434), ('1681c23tion', 435), ('1681d', 436), ('1681e', 437), ('1681g', 438), ('1681h', 439), ('1681i', 440), ('1681j', 441), ('1681m', 442), ('1681n', 443), ('1681o', 444), ('1681r', 445), ('1681s', 446), ('1681t', 447), ('1681u', 448), ('1681v', 449), ('1682', 450), ('1682g', 451), ('1688', 452), ('1689', 453), ('168i', 454), ('168li', 455), ('168s', 456), ('169', 457), ('1691', 458), ('1692', 459), ('1692a', 460), ('1692b', 461), ('1692c', 462), ('1692d', 463), ('1692dpennymac', 464), ('1692e', 465), ('1692f', 466), ('1692g', 467), ('1692i', 468), ('1692j', 469), ('1692k', 470), ('1692p', 471), ('1693', 472), ('1693a', 473), ('1693g', 474), ('1693n', 475), ('1693o', 476), ('1695', 477), ('169g', 478), ('169g2', 479), ('16cfr', 480), ('16cfr603', 481), ('16hrs', 482), ('16k', 483), ('16monthsdate', 484), ('16th', 485), ('16yrs', 486), ('17', 487), ('170', 488), ('1700', 489), ('17000', 490), ('170000', 491), ('1701j', 492), ('17101', 493), ('1715z', 494), ('17200', 495), ('173', 496), ('174', 497), ('1746', 498), ('1747', 499)]\n"
     ]
    }
   ],
   "source": [
    "print('vocab:')\n",
    "print(sorted(vocab.items())[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67652\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
